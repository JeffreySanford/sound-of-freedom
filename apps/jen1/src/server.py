from fastapi import FastAPI, Body
from pydantic import BaseModel
from typing import Optional, List, Dict
import uvicorn
import os
import logging
import boto3
import botocore
from urllib.parse import urlparse

app = FastAPI()

# Optionally import torch if environment requests GPU inference
HAS_TORCH = False
TORCH_DEVICE = None
MODEL_LOADED = False
MODEL_NAME = os.environ.get('JEN1_MODEL') or os.environ.get('MODEL_NAME')
MODEL_S3_PATH = os.environ.get('MODEL_S3_PATH')
MODEL_CACHE_DIR = os.environ.get('MODEL_CACHE_DIR') or '/workspace/models'
MODEL_INFO = None
LOGGER = logging.getLogger('jen1')
LOGGER.setLevel(logging.INFO)
handler = logging.StreamHandler()
LOGGER.addHandler(handler)
text_pipeline = None
try:
    if os.environ.get('USE_TORCH') == '1':
        import torch
        HAS_TORCH = True
        if torch.cuda.is_available():
            TORCH_DEVICE = 'cuda'
        else:
            TORCH_DEVICE = 'cpu'
        # Attempt to import transformers and preload model if configured
        if MODEL_NAME:
            try:
                from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
                device = 0 if TORCH_DEVICE == 'cuda' else -1
                LOGGER.info(f'Loading model {MODEL_NAME} to device {TORCH_DEVICE}')
                # If an S3 path is provided, download into MODEL_CACHE_DIR and use that local path
                if MODEL_S3_PATH and MODEL_S3_PATH.startswith('s3://'):
                    # e.g., s3://bucket/path/model.tar.gz or a model folder
                    try:
                        LOGGER.info(f'Downloading model from S3: {MODEL_S3_PATH}')
                        parsed = urlparse(MODEL_S3_PATH)
                        bucket = parsed.netloc
                        key = parsed.path.lstrip('/')
                        filename = os.path.basename(key)
                        local_dir = os.path.join(MODEL_CACHE_DIR, bucket, os.path.dirname(key))
                        local_path = os.path.join(local_dir, filename)
                        os.makedirs(local_dir, exist_ok=True)
                        s3 = boto3.client('s3')
                        if not os.path.exists(local_path):
                            s3.download_file(bucket, key, local_path)
                            LOGGER.info('Downloaded model to %s' % local_path)
                        # If local_path is a compressed archive, the user must expand it and set MODEL_NAME to the dir
                        MODEL_NAME = local_path
                    except botocore.exceptions.BotoCoreError as e:
                        LOGGER.warning('Failed to download model from S3: %s' % str(e))
                    except Exception as e:
                        LOGGER.warning('Failed to download model from S3: %s' % str(e))
                tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
                model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16 if TORCH_DEVICE == 'cuda' else None).to(TORCH_DEVICE)
                text_pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)
                MODEL_LOADED = True
                MODEL_INFO = { 'name': MODEL_NAME, 'device': TORCH_DEVICE }
            except Exception as e:
                LOGGER.warning(f'Failed to load model {MODEL_NAME}: {e}')
except Exception:
    HAS_TORCH = False
    TORCH_DEVICE = None

class GenerateRequest(BaseModel):
    narrative: str
    duration: int = 30
    options: Optional[Dict] = {}


@app.get('/', tags=["health"])
async def root():
    return {"name": "jen1", "status": "ready"}


@app.get('/health')
async def health():
    return {"status": "ok"}


@app.get('/debug/torch')
async def debug_torch():
    return {"has_torch": HAS_TORCH, "device": TORCH_DEVICE, 'model_loaded': MODEL_LOADED, 'model': MODEL_INFO}


@app.get('/debug/model')
async def debug_model():
    return { 'model_loaded': MODEL_LOADED, 'model_info': MODEL_INFO }


@app.post('/generate')
async def generate(req: GenerateRequest = Body(...)):
    narrative = req.narrative or "A gentle melody in the code"
    duration = req.duration or 30
    title = narrative.split('.')[0][:40] if narrative else 'Untitled Jen1 Song'
    lyrics = f"Generated by jen1 (FastAPI):\n{narrative}\nChorus: Remember the harmony"
    # If incoming options contain a pre-generated song structure, return it directly (PoC path)
    if req.options and req.options.get('generatedSong'):
        return req.options.get('generatedSong')

    song = {
        "title": title,
        "artist": "jen1 (PoC) Composer",
        "genre": "pop",
        "tempo": 120,
        "time_signature": "4/4",
        "key": "C major",
        "mood": "energetic",
        "instrumentation": ["piano", "guitar", "drums", "lead_voice"],
        "verse_1": {
            "lyrics": lyrics.split('\n')[:3],
            "chords": ["C", "G", "Am", "F"],
        },
        "chorus": {
            "lyrics": ["Remember the harmony", "Let the melody hold us"],
            "chords": ["F", "G", "Em", "Am"],
        },
        "syllableCount": len(lyrics.split()),
        "wordCount": len(lyrics.split()),
    }
    # If model pipeline loaded, generate lyrics using model
    if MODEL_LOADED:
        try:
            prompt = narrative
            # Limit tokens and keep generation concise
            out = text_pipeline(prompt, max_new_tokens=150, do_sample=True, num_return_sequences=1)
            if isinstance(out, list) and len(out) > 0 and 'generated_text' in out[0]:
                generated_text = out[0]['generated_text']
                song['lyrics'] = generated_text.split('\n')
                song['model'] = MODEL_INFO
        except Exception as e:
            LOGGER.warning('Model generation failed: %s' % str(e))
    # For PoC, add options echo
    if req.options:
        song['options'] = req.options
    return song


if __name__ == '__main__':
    port = int(os.environ.get('PORT', 4001))
    uvicorn.run(app, host='0.0.0.0', port=port)
